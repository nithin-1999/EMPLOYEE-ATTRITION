---
title: "90% acc and 89% AUC project code"
author: "NITHIN C"
date: "10/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

IBM HR Data Analysis- Employee Attrition
Introduction: To predict if an employee is going to resign or not

```{r}
getwd()
```


```{r}
library(data.table)
library(dplyr)
library(VIM)
library(DT)
library(gridExtra)
library(ggplot2)
library(caret)
library(Metrics)
library(randomForest)
library(pROC)
library(e1071)
library(dtree)
library(corrplot)
library(rpart.plot)
library(DMwR)
library(readr)

rm(list = ls())
```

required Libraries:


```{r}
getwd()
```

```{r}
myData <- read.csv('ATTRITION.csv',sep = ',',header = TRUE,stringsAsFactors = TRUE)
names(myData)
glimpse(myData)
summary(myData)
```

```{r}
par(mar= c(4, 4, 2, 2))


```
As we see in the Data:

Observations: 1,470 with Variables: 35

Class Label is Attrition with 1232 'NO' and 237 'Yes' that shows the unbalance class label. we have to pay attention to the unbalance class algorithm problems!

Employee Count is equal 1 for all observation which can not generate useful value for this sample data. Maybe for the other sample of data will be with different values that should be considered for builiding the model in the future for other sets of data. In this analysis, we will remove it.

Over 18 is equal to 'Y', which means employee is not less than 18 years old. this attribute should be considered for the future, maybe by changing the ruls of emploement, young people under 18 can also working in companies. Here, according to the data set, we will remove it.

Moreover, Standard Hours is equal 80 for all observation. the decision for this attribute is same to Over18 and Employee Count. BusinessTravel, Department, EducationField, Gender, jobRole, MaritalStatus and OverTime are categorical data and other variabels are continues.

Some of variables are related to the years of working wich can be a good candidate for feature generation. Some of variable are related to personal issues like WorkLifeBalance, RelationshipSatisfaction, JobSatisfaction,EnvironmentSatisfaction etc.

There are some variables that are related to the income like MonthlyIncome, PercentSalaryHike, etc.

EmployeeNumber is a variable for identifying the specific employee.If we have more information about employee and the structure of the employee number, then we can extract some new features. But now it is not possible and we have to remove it from our data set.

More and more, we have to envestigate that, how the company objective factors influence in attition employees, and what kind of working enviroment most will cause employees attrition.

Check for Missing values

```{r}

apply(is.na(myData), 2, sum)
VIM::aggr(myData)

```

No Missing Value, we are lucky
Remove non value attributes

These variables can not play significant role because they are same for all records.

also, EmployeeNumber can be accepted as an indicator for the time of join to the company which can be used for new feature generation,But we do not have any meta data about it, then, we will remove it.

```{r}
cat("Data Set has ",dim(myData)[1], " Rows and ", dim(myData)[2], " Columns" )
myData$EmployeeNumber<- NULL
myData$StandardHours <- NULL
myData$Over18 <- NULL
myData$EmployeeCount <- NULL
cat("Data Set has ",dim(myData)[1], " Rows and ", dim(myData)[2], " Columns" )

sum (is.na(duplicated(myData)))
```

There are some attributes that are categorical, but in the data set are integer. We have to change them to categorical. also, we do not need any dummy variable creation, where some machine learning algorithms like RF, XGBoost etc. can use categorical variables.

For other algorithms like NN we have to change categorical variable more than two level to dummy variable Variable with twol level (Binary) can be change to number very easy.


```{r fig.height=400, fig.width=800, warning=TRUE, paged.print=TRUE}
myData$Education <- factor(myData$Education)
myData$EnvironmentSatisfaction <- factor(myData$EnvironmentSatisfaction)
myData$JobInvolvement <- factor(myData$JobInvolvement)
myData$JobLevel <- factor(myData$JobLevel)
myData$JobSatisfaction <- factor(myData$JobSatisfaction)
myData$PerformanceRating <- factor(myData$PerformanceRating)
myData$RelationshipSatisfaction <- factor(myData$RelationshipSatisfaction)
myData$StockOptionLevel <- factor(myData$StockOptionLevel)
myData$WorkLifeBalance <- factor(myData$WorkLifeBalance)
```
Visualization of Attrition


```{r}
capabilities()

```

```{r echo=TRUE, fig.height=600, fig.width=800}
myData %>%
        group_by(Attrition) %>%
        tally() %>%
        ggplot(aes(x = Attrition, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        labs(x="Attrition", y="Count of Attriation")+
        ggtitle("Attrition")+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))

myData
```

As we see here, 237/1470=0.16 % of the data label shows the "Yes" in Attrition. this problem should be handeled during the process because unbalanced dataset will bias the prediction model towards the more common class (here is 'NO'). There are different approaches for dealing with unbalanced data in machine learning like using more data (here is not possible), Resampling , changing the machine performance metric, using various algorithms etc.

```{r}
ggplot(data=myData, aes(myData$Age,count()) )+
         
geom_histogram(breaks=seq(20, 50, by=2), 
                       col="red", 
                       aes(fill=..count..))+
        labs(x="Age", y="Count")+
        scale_fill_gradient("Count", low="green", high="red")
```


as we see above, the majority of employees are between 28-36 years. 34-36 years old are very popolar.



```{r}
a1 <- myData %>%
        group_by(BusinessTravel) %>%
        tally() %>%
        ggplot(aes(x = BusinessTravel, y = n,fill=BusinessTravel)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        labs(x="Business Travel", y="Number Attriation")+
        ggtitle("Attrition according to the Business Travel")+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))

a2<- myData %>%
        group_by(BusinessTravel, Attrition) %>%
        tally() %>%
        ggplot(aes(x = BusinessTravel, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        labs(x="Business Travel", y="Number Attriation")+
        ggtitle("Attrition according to the Business Travel")+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))


grid.arrange(a1,a2)
```

Here is the distribution of the data according to the Business Tralvel situation. more than 70% of employees travel rarely where just 10 % of them has no trave


```{r}
myData %>%
        ggplot(aes(x = BusinessTravel, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = 2) +
        labs(y = "Percentage", fill= "business Travel") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) + 
        ggtitle("Attrition")
```



```{r}
g1 <- myData %>%
        group_by(Department) %>%
        tally() %>%
        ggplot(aes(x = Department, y = n,fill=Department)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        geom_text(aes(label = n), vjust = -0.1, position = position_dodge(0.9))

g2 <- myData %>%
        group_by(Department, Attrition) %>%
        tally() %>%
        ggplot(aes(x = Department, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        geom_text(aes(label = n), vjust = -0.1, position = position_dodge(0.9))

grid.arrange(g1,g2)
```


```{r}
g1<- myData %>%
        ggplot(aes(x = Education, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = 2) +
        labs(y = "Percentage", fill= "Education") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) + 
        ggtitle("Attrition")

g2<- myData %>%
        group_by(Education, Attrition) %>%
        tally() %>%
        ggplot(aes(x = Education, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))+
        labs(x="Education", y="Number Attriation")+
        ggtitle("Attrition in regards to Education Level")

grid.arrange(g1,g2)
```


```{r}
myData %>%
        ggplot(aes(x = Gender, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = -.5) +
        labs(y = "Percentage", fill= "Gender") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) + 
        ggtitle("Attrition")
```

```{r}
myData %>%
        ggplot(aes(x = MaritalStatus, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = -.5) +
        labs(y = "Percentage", fill= "MaritalStatus") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) + 
        ggtitle("Attrition")
```

```{r}
myData %>%
        ggplot(mapping = aes(x = MonthlyIncome)) + 
        geom_histogram(aes(fill = Attrition), bins=20)+
        labs(x="Monthlt Income", y="Number Attriation")+
        ggtitle("Attrition in regards to Monthly Income")
```


```{r}
g1 <-myData %>%
        ggplot(aes(x = OverTime, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = 0.3) +
        labs(y = "Percentage", fill= "OverTime") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.3)) + 
        ggtitle("Attrition")


g2 <-myData %>%
        group_by(OverTime, Attrition) %>%
        tally() %>%
        ggplot(aes(x = OverTime, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        geom_text(aes(label = n), vjust = -0.3, position = position_dodge(0.9))+
        labs(x="Over time", y="Number Attriation")+
        ggtitle("Attrition in regards to Over time")

grid.arrange(g1,g2)
```


```{r}
g1<-myData %>%
        ggplot(aes(x = WorkLifeBalance, group = Attrition)) + 
        geom_bar(aes(y = ..prop.., fill = factor(..x..)), 
                 stat="count", 
                 alpha = 0.7) +
        geom_text(aes(label = scales::percent(..prop..), y = ..prop.. ), 
                  stat= "count", 
                  vjust = -.5) +
        labs(y = "Percentage", fill= "WorkLifeBalance") +
        facet_grid(~Attrition) +
        theme_minimal()+
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) + 
        ggtitle("Attrition")

g2<- myData %>%
        group_by(WorkLifeBalance, Attrition) %>%
        tally() %>%
        ggplot(aes(x = WorkLifeBalance, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))+
        labs(x="  Work Life Balance", y="Number Attriation")+
        ggtitle("Attrition in regards to  Work Life Balance")
grid.arrange(g1,g2)
```


Using Raw data by RF
At the first Stage we use RF for getting some information about the prediction split Data to Train and Test:


```{r}
rfData <- myData
set.seed(123)




indexes = sample(1:nrow(rfData), size=0.7*nrow(rfData))
RFRaw.train.Data <- rfData[indexes,]
RFRaw.test.Data <- rfData[-indexes,]
```

Building the model

```{r}
Raw.rf.model <- randomForest(Attrition~.,RFRaw.train.Data, importance=TRUE,ntree=1000)
varImpPlot(Raw.rf.model)
```

As we see here, Over time, Age, MonthlyIncome, Jobrole and TotalWorkingYears are top five variables.



```{r}
Raw.rf.prd <- predict(Raw.rf.model, newdata = RFRaw.test.Data)
confusionMatrix(RFRaw.test.Data$Attrition, Raw.rf.prd)
```




```{r}
Raw.rf.plot<- plot.roc(as.numeric(RFRaw.test.Data$Attrition), as.numeric(Raw.rf.prd),lwd=2, type="b",print.auc=TRUE,col ="blue")
```

Acc = 0.8639 which is very good result but not anough measure. We see that the AUC is poor.

Feature Engineering
Now we want to use some data wrapping to make the results better:

Making Age Group 18-24 = Young , 25-54=Middle-Age , 54-120= Adult



```{r}
myData$AgeGroup <- as.factor(
        ifelse(myData$Age<=24,"Young", ifelse(
        myData$Age<=54,"Middle-Age","Adult"
        ))
)
table(myData$AgeGroup)
```

as we see the majority of employee are Young

2- Totla Satisfaction the total of the satisfaction from Job, Environment, etc.


```{r}
myData$TotlaSatisfaction <- 
        as.numeric(myData$EnvironmentSatisfaction)+
        as.numeric(myData$JobInvolvement)+
        as.numeric(myData$JobSatisfaction)+
        as.numeric(myData$RelationshipSatisfaction)+
        as.numeric(myData$WorkLifeBalance)

summary(myData$TotlaSatisfaction)
```

3- Study Years for getting Education Level certificate

```{r}
table(myData$Education)
# As we see here, there are five Education level
# From high School to PhD (HighSchool=10 years, College=2 years, Bachelor=4 years,Master=2 years,PhD= four years)
# we used culumative years for any level
```
```{r}
myData$YearsEducation <-  ifelse(myData$Education==1,10,ifelse(myData$Education==2,12,
        ifelse(myData$Education==3,16,ifelse(myData$Education==4,18,22))))  

table(myData$YearsEducation)
# the majority of employee are 16 years education (Bachelor)
```
4- Less or more than average Monthly Income We calculate the average income and generate the level of incom(High or Low)


```{r}
myData$IncomeLevel <- as.factor(
        ifelse(myData$MonthlyIncome<ave(myData$MonthlyIncome),"Low","High")
)
table(myData$IncomeLevel)
```

Let see the Correlation Matrix of Data

```{r}
corrplot(cor(sapply(myData,as.integer)),method = "pie")
```

We can see some of variables are high correlated
As an Example :

           JobLevel and MonthlyIncome
           Education and YearsEducation

they cause multicollinearity problem in our data set! we have to deciede to remove one of them for any group Now we try again our data set with new attributes using Random Forest

New Random Forest

```{r}
rfData <- myData
set.seed(123)
indexes = sample(1:nrow(rfData), size=0.8*nrow(rfData))
RFtrain.Data <- rfData[indexes,]
RFtest.Data <- rfData[-indexes,]

rf.model <- randomForest(Attrition~.,RFtrain.Data, importance=TRUE,ntree=500)
varImpPlot(rf.model)
```

Here we see: OverTime, TotalSatisfaction (Generated Attr.), MonthlyIncome, Age and TotalWorkingYears are top five variables.


```{r}
rf.prd <- predict(rf.model, newdata = RFtest.Data)
confusionMatrix(RFtest.Data$Attrition, rf.prd)
```

```{r}
rf.Plot<- plot.roc (as.numeric(RFtest.Data$Attrition), as.numeric(rf.prd),lwd=2, type="b", print.auc=TRUE,col ="blue")
```


It is better than FR algorithm with raw data.

TotalSatisfaction is high important attribute

Let using other algorithms

Using Support Vector MachineÂ¶


```{r}
svmData <- myData
set.seed(123)
indexes = sample(1:nrow(svmData), size=0.8*nrow(svmData))
SVMtrain.Data <- svmData[indexes,]
SVMtest.Data <- svmData[-indexes,]
tuned <- tune(svm,factor(Attrition)~.,data = SVMtrain.Data)
svm.model <- svm(SVMtrain.Data$Attrition~., data=SVMtrain.Data
                 ,type="C-classification", gamma=tuned$best.model$gamma
                 ,cost=tuned$best.model$cost
                 ,kernel="radial")
svm.prd <- predict(svm.model,newdata=SVMtest.Data)
confusionMatrix(svm.prd,SVMtest.Data$Attrition)
```

```{r}
svm.plot <-plot.roc (as.numeric(SVMtest.Data$Attrition), as.numeric(svm.prd),lwd=2, type="b", print.auc=TRUE,col ="blue")
```


as we see, in compare to RF, Accuracy is lower to 0.8571 and AUC to 0.571 which is not better than RF.

There is no False Negative! and a lot of False Positive!

Decision Tree
Here Dtree will be investigated and compared to other approaches. DTree selected because it is a very good algorithm for interpretion for non-technical like HR.


`
```{r}
DtData <- myData
set.seed(123)
indexes = sample(1:nrow(DtData), size=0.8*nrow(DtData))
DTtrain.Data <- DtData[indexes,]
DTtest.Data <- DtData[-indexes,]

dtree.model <- dtree::dtree (Attrition ~., data = DTtrain.Data)
plot(dtree.model)
text(dtree.model, all = T)
```
```{r}
dtree.prd <- predict(dtree.model, DTtest.Data, type = "class")
confusionMatrix(dtree.prd,DTtest.Data$Attrition)
```

```{r}
dtree.plot <- plot.roc (as.numeric(DTtest.Data$Attrition), as.numeric(dtree.prd),lwd=2, type="b", print.auc=TRUE, col ="blue")
```


not very nice result! Accuracy is 0.823 where AUC is 0.608 which is better than SVM Always we can not get better result from RF instead of DTree, but here it is a nice result.

Exterme Gradient Boost


```{r}
set.seed(123)
xgbData <- myData
indexes <- sample(1:nrow(xgbData), size=0.8*nrow(xgbData))
XGBtrain.Data <- xgbData[indexes,]
XGBtest.Data <- xgbData[-indexes,]

formula = Attrition~.
fitControl <- trainControl(method="cv", number = 3,classProbs = TRUE )
xgbGrid <- expand.grid(nrounds = 50,
                       max_depth = 12,
                       eta = .03,
                       gamma = 0.01,
                       colsample_bytree = .7,
                       min_child_weight = 1,
                       subsample = 0.9
)
```

```{r}
XGB.model <- train(formula, data = XGBtrain.Data,
                  method = "xgbTree"
                  ,trControl = fitControl
                  , verbose=0
                  , maximize=FALSE
                  ,tuneGrid = xgbGrid
)
```

```{r}
importance <- varImp(XGB.model)
varImportance <- data.frame(Variables = row.names(importance[[1]]), 
                            Importance = round(importance[[1]]$Overall,2))
# Create a rank variable based on importance of variables
rankImportance <- varImportance %>%
        mutate(Rank = paste0('#',dense_rank(desc(Importance))))
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
                           y = Importance)) +
        geom_bar(stat='identity',colour="white", fill = "lightgreen") +
        geom_text(aes(x = Variables, y = 1, label = Rank),
                  hjust=0, vjust=.5, size = 4, colour = 'black',
                  fontface = 'bold') +
        labs(x = 'Variables', title = 'Relative Variable Importance') +
        coord_flip() + 
        theme_bw()
```
As we see above: MonthlyIncome, DailyRate, OvertimeYes, TotalSatisfaction and Age are top five attributes.

```{r}
XGB.prd <- predict(XGB.model,XGBtest.Data)
confusionMatrix(XGB.prd, XGBtest.Data$Attrition)
```


```{r}
XGB.plot <- plot.roc (as.numeric(XGBtest.Data$Attrition), as.numeric(XGB.prd),lwd=2, type="b", print.auc=TRUE,col ="blue")
```

As we see the ACC is 0.8707 which is good result.

Perfect: the best result is in Balanced Accuracy which is 0.629

Solving Unbalanced Problem
As we mentioned before there is unbalanced problem in class lable. There are some approaches to solve the problem. here we use SMOT method.

```{r}
Classcount = table(myData$Attrition)
# Over Sampling
over = ( (0.6 * max(Classcount)) - min(Classcount) ) / min(Classcount)
# Under Sampling
under = (0.4 * max(Classcount)) / (min(Classcount) * over)

over = round(over, 1) * 100
under = round(under, 1) * 100
#Generate the balanced data set

BalancedData = SMOTE(Attrition~., myData, perc.over = over, k = 5, perc.under = under)
# let check the output of the Balancing
BalancedData %>%
        group_by(Attrition) %>%
        tally() %>%
        ggplot(aes(x = Attrition, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        labs(x="Attrition", y="Count of Attriation")+
        ggtitle("Attrition")+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```



```{r}
# Now we try to run again XGBoost with the Balanced Data
set.seed(123)
xgbData <- BalancedData
indexes = sample(1:nrow(xgbData), size=0.8*nrow(xgbData))
BLtrain.Data <- xgbData[indexes,]
BLtest.Data <- xgbData[-indexes,]

formula = Attrition~.
fitControl <- trainControl(method="cv", number = 3,classProbs = TRUE )
xgbGrid <- expand.grid(nrounds = 500,
                       max_depth = 20,
                       eta = .03,
                       gamma = 0.01,
                       colsample_bytree = .7,
                       min_child_weight = 1,
                       subsample = 0.9
)

XGB.model = train(formula, data = BLtrain.Data,
                  method = "xgbTree"
                  ,trControl = fitControl
                  , verbose=0
                  , maximize=FALSE
                  ,tuneGrid = xgbGrid
                  ,na.action = na.pass
)
```

```{r}
importance <- varImp(XGB.model)
varImportance <- data.frame(Variables = row.names(importance[[1]]), 
                            Importance = round(importance[[1]]$Overall,2))
# Create a rank variable based on importance
rankImportance <- varImportance %>%
        mutate(Rank = paste0('#',dense_rank(desc(Importance))))
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
                           y = Importance)) +
        geom_bar(stat='identity',colour="white", fill = "lightgreen") +
        geom_text(aes(x = Variables, y = 1, label = Rank),
                  hjust=0, vjust=.5, size = 4, colour = 'black',
                  fontface = 'bold') +
        labs(x = 'Variables', title = 'Relative Variable Importance') +
        coord_flip() + 
        theme_bw()
```

```{r}
NewXGB.prd <- predict(XGB.model,BLtest.Data)
confusionMatrix(NewXGB.prd, BLtest.Data$Attrition)
```


```{r}
XGB.plot <- plot.roc (as.numeric(BLtest.Data$Attrition), as.numeric(NewXGB.prd),lwd=2, type="b", print.auc=TRUE, col ="blue")
```


Congratulation !
Excelent Results:
Accuracy : more than 90% !!!!!
AUC about 0.89
Here we plot all three approaches in one plot


```{r}
par(mfrow=c(2,3))
plot.roc (as.numeric(XGBtest.Data$Attrition), as.numeric(XGB.prd),main="XGBoost",lwd=2, type="b", print.auc=TRUE, col ="blue")
plot.roc (as.numeric(DTtest.Data$Attrition), as.numeric(dtree.prd), main="DTree",lwd=2, type="b", print.auc=TRUE, col ="brown")
plot.roc (as.numeric(BLtest.Data$Attrition), as.numeric(NewXGB.prd),main="New XGBoost",lwd=2, type="b", print.auc=TRUE, col ="green")
plot.roc (as.numeric(SVMtest.Data$Attrition), as.numeric(svm.prd),main="SVM",lwd=2, type="b", print.auc=TRUE, col ="red")
plot.roc (as.numeric(RFRaw.test.Data$Attrition), as.numeric(Raw.rf.prd), main="Random Forest",lwd=2, type="b", print.auc=TRUE, col ="seagreen4")
plot.roc (as.numeric(RFtest.Data$Attrition), as.numeric(rf.prd), main="Raw Data Random Forest",lwd=2, type="b", print.auc=TRUE, col ="seagreen")
```

New XGB algorithm selected...
Thanks for your consideration

I am very happy to leran more from your nice comments

Good luck.....


